{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1555766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YouTube Emotion Scraper (WORKING IN 2025)\n",
    "# Tanpa API Key â€¢ Tanpa yt-dlp Search\n",
    "# ============================================\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup\n",
    "import yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37dff43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1. HTML SEARCH (no API key)\n",
    "# ------------------------------\n",
    "\n",
    "def search_youtube(keyword, max_results=40):\n",
    "    query = keyword.replace(\" \", \"+\")\n",
    "    url = f\"https://www.youtube.com/results?search_query={query}\"\n",
    "\n",
    "    print(f\"Searching YouTube: {keyword}\")\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        html = r.text\n",
    "\n",
    "        # Extract video IDs from HTML\n",
    "        video_ids = re.findall(r\"watch\\?v=([a-zA-Z0-9_-]{11})\", html)\n",
    "\n",
    "        # Remove duplicates\n",
    "        unique_ids = []\n",
    "        seen = set()\n",
    "\n",
    "        for vid in video_ids:\n",
    "            if vid not in seen:\n",
    "                seen.add(vid)\n",
    "                unique_ids.append(vid)\n",
    "\n",
    "        print(f\" -> Found {len(unique_ids)} videos\")\n",
    "\n",
    "        return unique_ids[:max_results]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Search error:\", e)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70178a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2. SCRAPE COMMENTS\n",
    "# ------------------------------\n",
    "\n",
    "def scrape_comments(video_id, max_comments=10000):\n",
    "    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "        \"extract_flat\": True,\n",
    "        \"getcomments\": True,\n",
    "    }\n",
    "\n",
    "    print(f\"   Scraping comments from video {video_id} ...\")\n",
    "\n",
    "    comments = []\n",
    "\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "\n",
    "        if 'comments' in info:\n",
    "            for c in info['comments']:\n",
    "                if \"text\" in c:\n",
    "                    comments.append(c[\"text\"])\n",
    "\n",
    "                if len(comments) >= max_comments:\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Error scraping comments for {video_id}: {e}\")\n",
    "\n",
    "    print(f\"   -> Collected {len(comments)} comments\")\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfeb7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3. SAVE CSV\n",
    "# ------------------------------\n",
    "\n",
    "def save_csv(filename, rows):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"comment\"])\n",
    "        writer.writerows([[r] for r in rows])\n",
    "    print(f\"Saved {len(rows)} comments to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71844bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4. EMOTION KEYWORDS\n",
    "# ------------------------------\n",
    "\n",
    "emotion_keywords = {\n",
    "    \"sadness\": [\"sad song\", \"broken heart\", \"sad story\", \"emotional music\"],\n",
    "    \"anger\": [\"angry rant\", \"rage compilation\", \"angry moments\"],\n",
    "    \"fear\": [\"scary story\", \"horror story\", \"creepy videos\"],\n",
    "    \"joy\": [\"happy moments\", \"funny video\", \"joyful moments\"],\n",
    "    \"love\": [\"love song\", \"romantic story\", \"relationship advice\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c62970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_emotion_comments(title, keywords=None, target_count=10000):\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\" Collecting EMOTION: {title}\")\n",
    "    print(f\"============================\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 1. Gunakan 1 video saja\n",
    "    # ------------------------------------\n",
    "    video_id = \"vcsSc2iksC0\"\n",
    "    print(f\"Scraping comments from video: {video_id}\")\n",
    "\n",
    "    # Ambil komentar sebanyak mungkin\n",
    "    all_comments = scrape_comments(video_id, max_comments=target_count)\n",
    "\n",
    "    print(f\"Total comments collected: {len(all_comments)}\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 2. Simpan ke CSV\n",
    "    # ------------------------------------\n",
    "    file_name = f\"{title}.csv\"\n",
    "    save_csv(file_name, all_comments)\n",
    "\n",
    "    print(f\"Saved {len(all_comments)} comments to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a86653ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      " Collecting EMOTION: Life_in_a_Day_2020_Official_Documentary\n",
      "============================\n",
      "Scraping comments from video: vcsSc2iksC0\n",
      "   Scraping comments from video vcsSc2iksC0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n",
      "WARNING: [youtube] vcsSc2iksC0: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] vcsSc2iksC0: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Collected 509 comments\n",
      "Total comments collected: 509\n",
      "Saved 509 comments to Life_in_a_Day_2020_Official_Documentary.csv\n",
      "Saved 509 comments to Life_in_a_Day_2020_Official_Documentary.csv\n",
      "\n",
      "=== ALL DONE! ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RUN ALL EMOTIONS\n",
    "# ============================================\n",
    "\n",
    "title = \"Life_in_a_Day_2020_Official_Documentary\"\n",
    "\n",
    "\n",
    "# for emotion, keywords in emotion_keywords.items():\n",
    "collect_emotion_comments(title,  target_count=10000)\n",
    "\n",
    "print(\"\\n=== ALL DONE! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca026649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ini keyword ee\n",
    "\n",
    "# Uncomment kalo mau jalanin scrap\n",
    "# emotion = \"sadness\"\n",
    "# emotion = \"anger\"\n",
    "# emotion = \"fear\"\n",
    "# emotion = \"joy\"\n",
    "# emotion = \"love\"\n",
    "\n",
    "# queries = {\n",
    "#     \"sadness\":  '\"I feel sad\" OR \"I am sad\" OR \"so sad\" OR \"heartbroken\" OR \"feeling down\" OR \"I feel depressed\"',\n",
    "#     \"anger\":    '\"I am angry\" OR \"so mad\" OR \"pissed off\" OR \"furious\" OR \"so angry\" OR \"I hate this\"',\n",
    "#     \"fear\":     '\"I am scared\" OR \"I feel anxious\" OR \"terrified\" OR \"I am afraid\" OR \"panic attack\"',\n",
    "#     \"joy\":      '\"I feel happy\" OR \"so happy\" OR \"excited\" OR \"grateful\" OR \"I am joyful\" OR \"good news\"',\n",
    "#     \"love\":     '\"I love you\" OR \"I love this\" OR \"so in love\" OR \"I adore\" OR \"I really love\"'\n",
    "# }\n",
    "# keyword = queries[emotion]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
